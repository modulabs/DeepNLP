On Polynomial time Constructions of
Minimum Height Decision Tree
Nader H. Bshouty Waseem Makhoul
Technion, Haifa, Israel
bshouty@ca.technion.ac.il
Abstract. A decision tree T in Bm := {0, 1}m is a binary tree where
each of its internal nodes is labeled with an integer in [m] = {1, 2, . . . , m},
each leaf is labeled with an assignment a âˆˆ Bm and each internal node
has two outgoing edges that are labeled with 0 and 1, respectively. Let
A âŠ‚ {0, 1}m. We say that T is a decision tree for A if (1) For every a âˆˆ A
there is one leaf of T that is labeled with a. (2) For every path from the
root to a leaf with internal nodes labeled with i1, i2, . . . , ik âˆˆ [m], a leaf
labeled with a âˆˆ A and edges labeled with Î¾i1 , . . . , Î¾ik âˆˆ {0, 1}, a is the
only element in A that satisï¬es aij = Î¾ij for all j = 1, . . . , k.
Our goal is to write a polynomial time (in n := |A| and m) algorithm
that for an input A âŠ† Bm outputs a decision tree for A of minimum
depth. This problem has many applications that include, to name a few,
computer vision, group testing, exact learning from membership queries
and game theory.
Arkin et al. and Moshkov [4,14] gave a polynomial time (ln|A|)- approx-
imation algorithm (for the depth). The result of Dinur and Steurer [6] for
set cover implies that this problem cannot be approximated with ratio
(1âˆ’ o(1))Â· ln|A|, unless P=NP. Moskov studied in [14] the combinatorial
measure of extended teaching dimension of A, ETD(A). He showed that
ETD(A) is a lower bound for the depth of the decision tree for A and
then gave an exponential time ETD(A)/ log(ETD(A))-approximation al-
gorithm.
In this paper we further study the ETD(A) measure and a new com-
binatorial measure, DEN(A), that we call the density of the set A. We
show that DEN(A) â‰¤ ETD(A) + 1. We then give two results. The ï¬rst
result is that the lower bound ETD(A) of Moshkov for the depth of the
decision tree for A is greater than the bounds that are obtained by the
classical technique used in the literature. The second result is a poly-
nomial time (ln 2)DEN(A)-approximation (and therefore (ln 2)ETD(A)-
approximation) algorithm for the depth of the decision tree of A. We
also show that a better approximation ratio implies P=NP.
We then apply the above results to learning the class of disjunctions
of predicates from membership queries [5]. We show that the ETD of
this class is bounded from above by the degree d of its Hasse diagram.
We then show that Moshkov algorithm can be run in polynomial time
and is (d/ log d)-approximation algorithm. This gives optimal algorithms
when the degree is constant. For example, learning axis parallel rays over
constant dimension space.
2
Introduction
Consider the following problem: Given an n-element set A âŠ† Bm := {0, 1}m
from some class of sets A and a hidden element a âˆˆ A. Given an oracle that
answers queries of the type: â€œWhat is the value of ai?â€. Find a polynomial time
algorithm that with an input A, asks minimum number of queries to the oracle
and ï¬nds the hidden element a. This is equivalent to constructing a minimum
height decision tree for A. A decision tree is a binary tree where each internal
node is labeled with an index from [m] and each leaf is labeled with an assignment
a âˆˆ Bm. Each internal node has two outgoing edges one that is labeled with 0
and the other is labeled with 1. A node that is labeled with i corresponds to the
query â€œIs ai = 0?â€. An edge that is labeled with Î¾ corresponds to the answer
Î¾. This decision tree is an algorithm in an obvious way and its height is the
worst case complexity of the number of queries. A decision tree T is said to be a
decision tree for A if the algorithm that corresponds to T predicts correctly the
hidden assignment a âˆˆ A. Our goal is to construct a small height decision tree
for A âŠ† Bm in time polynomial in m and n := |A|. We will denote by OPT(A)
the minimum height decision tree for A.
This problem is related to the following problem in exact learning [1]: Given
a class C of boolean functions f : X â†’ {0, 1}. Construct in poly(|C|,|X|) time
an optimal adaptive algorithm that learns C from membership queries. This
learning problem is equivalent to constructing a minimum height decision tree
j = fi(xj)} where fi is the ith function in C and xj is
for the set A = {a(i)|a(i)
the jth instance in X. In computer vision the problem is related to minimizing
the number of â€œprobesâ€ (queries) needed to determine which one of a ï¬nite set
of geometric ï¬gures is present in an image [4]. In game theory the problem is
related to the minimum number of turns required in order to win a guessing
game.
1.1 Previous and New Results
In [4], Arkin et al. showed that (AMMRS-algorithm) if at every node the deci-
sion tree chooses i that partitions the current set (the set of assignments that
are consistent to the answers of the queries so far) as evenly as possible, then
the height of the tree is within a factor of log |A| from optimal. I.e., log |A|-
approximation algorithm. Moshkov [14] analysis shows that this algorithm is
(ln|A|)-approximation algorithm. This algorithm runs in polynomial time in m
and |A|.
Hyaï¬l and Rivest, [11], show that the problem of constructing a minimum
depth decision tree is NP-Hard. The reduction of Laber and Nogueira, [12] to
set cover with the inapproximability result of Dinur and Steurer [6] for set cover
implies that it cannot be approximated to a factor of (1 âˆ’ o(1)) Â· ln|A| unless
P=NP. Therefore, no better approximation ratio can be obtained if no constraint
is added to the set A.
Moshkov, [13], studied the extended teaching dimension combinatorial mea-
sure, ETD(A), of a set A âŠ† Bm. It is the maximum over all the possible assign-
3
ments b âˆˆ Bm of the minimum number of indices I âŠ‚ [m] in which b agrees with
at most one a âˆˆ A. Moshkov showed two results. The ï¬rst is that ETD(A) is a
lower bound for OPT(A). The second is an exponential time algorithm that asks
(2ETD(A)/ log ETD(A)) log n queries. This gives a (ln 2) (ln|A|)/ log ETD(A) -
approximation (exponential time) algorithm (since OPT(A) â‰¥ ETD(A)) and at
the same time 2ETD (A)/ log ETD(A)-approximation algorithm (since OPT(A) â‰¥
log |A|). Since many interesting classes have small ETD dimension, the latter re-
sult gives small approximation ratio but unfortunately Moshkov algorithm runs
in exponential time.
In this paper we further study the ETD measure. We show that any poly-
nomial time (1 âˆ’ o(1))ETD(C)-approximation algorithm implies P=NP. There-
fore, Moshkov algorithm cannot run in polynomial time unless P=NP. We then
show that the above AMMRS-algorithm, [4], is polynomial time (ln 2)ETD(C)-
approximation algorithm. This gives a small approximation ratio for classes with
small extended teaching dimension.
Another reason for studying the ETD of classes is the following: If you ï¬nd
the ETD of the set A then you either get a lower bound that is better than the
information theoretic lower bound log |A| or you get an approximation algorithm
with a better ratio than ln|A|. This is because if ETD(A) < log |A| then the
AMMRS-algorithm has a ratio (ln 2)ETD(A) that is better than the ln|A| ratio
and if ETD(A) > log |A| then Moshkov lower bound, ETD(A), for OPT(A) is
better than the information theoretic lower bound log |A|.
To get the above results, we deï¬ne a new combinatorial measure called the
density DEN(A) of the set A. If Q = DEN(A) then there is a subset B âŠ† A such
that an adversary can give answers to the queries that eliminate at most 1/Q
fraction of the number of elements in B. This forces the learner to ask at least
Q queries. We then show that ETD(A) â‰¥ DEN(A) âˆ’ 1. On the other hand, we
show that if Q = DEN(A) then a query in the AMMRS-algorithm eliminates
at least (1 âˆ’ 1/Q) fraction of the assignments in A. This gives a polynomial
time (ln 2)DEN(A)-approximation algorithm which is also a (ln 2)(ETD(A) + 1)-
approximation algorithm.
In order to compare both algorithms we show that (ETD(A) âˆ’ 1)/ ln|A| â‰¤
DEN(A) â‰¤ ETD(A) + 1 and for random uniform A (and therefore for almost all
A), with high probability DEN(A) = Î˜(ETD(A)/ ln|A|). Since |A| > ETD(A),
this shows that AMMRS-algorithm may get a better approximation ratio than
Moshkov algorithm.
The inapproximability results follows from the reduction of Laber and Nogueira,
[12] to set cover with the inapproximability result of Dinur and Steurer [6] and
the fact that DEN(A) â‰¤ ETD(A) + 1 â‰¤ OPT(A) + 1.
We then apply the above results to learning the class of disjunctions of pred-
icates from a set of predicates F from membership queries [5]. We show that the
ETD of this class is bounded from above by the degree d of its Hasse diagram. We
then show that Moshkov algorithm, for this class, runs in polynomial time and
is (d/ log d)-approximation algorithm. Since |F| â‰¥ d (and in many applications,
|F| (cid:29) d), this improves the |F|-approximation algorithm SPEX in [5] when the
4
size of Hasse diagram is polynomial. This also gives optimal algorithms when
the degree d is constant. For example, learning axis parallel rays over constant
dimension space.
2 Deï¬nitions and Preliminary Results
In this section we give some deï¬nitions and preliminary results
2.1 Notation
Let Bm = {0, 1}m. Let A = {a(1), . . . , a(n)} âŠ† Bm be an n-element set. We
will write |A| for the number of elements in A. For h âˆˆ Bm we deï¬ne A + h =
{a + h|a âˆˆ A} where + (in the square brackets) is the bitwise exclusive or of
elements in Bm.
For integer q let [q] = {1, 2, . . . , q}. Throughout the paper, log x = log2 x.
2.2 Optimal Algorithm
We denote by OPT(A) the minimum depth of a decision tree for A. Our goal is
to build a decision tree for A with small depth.
Obviously
where n := |A|.
log n â‰¤ OPT(A) â‰¤ n âˆ’ 1
(1)
The following result is easy to prove (see Appendix A)
Lemma 1. We have OPT(A) = OPT(A + h).
2.3 Extended Teaching Dimension
In this section we deï¬ne the extended teaching dimension.
Let h âˆˆ Bm be any element. We say that a set S âŠ† [m] is a specifying set
for h with respect to A if |{a âˆˆ A | (âˆ€i âˆˆ S)hi = ai}| â‰¤ 1. That is, there is at
most one element in A that is consistent with h on the entries of S. Denote by
ETD(A, h) the minimum size of a specifying set for h with respect to A. The
extended teaching dimension of A is
ETD(A) = max
hâˆˆBm
ETD(A, h).
(2)
We will write ETDz(A) for ETD(A, 0). It is easy to see that
ETD(A, h) = ETDz(A + h) and ETD(A) = ETD(A + h).
(3)
We say that a set S âŠ† [m] is a strong specifying set for h with respect to
A if either h âˆˆ A and |{a âˆˆ A | (âˆ€i âˆˆ S)hi = ai}| = 1, or |{a âˆˆ A | (âˆ€i âˆˆ
5
S)hi = ai}| = 0. That is, if h âˆˆ A then there is exactly one element in A that
is consistent with h on the entries of S. Otherwise, no element in A is consistent
with h on S. Denote SETD(A, h) the minimum size of a strong specifying set
for h with respect to A. The strong extended teaching dimension of A is
SETD(A) = max
hâˆˆBm
SETD(A, h).
(4)
We will write SETDz(A) for SETD(A, 0). It is easy to see that
SETD(A, h) = SETDz(A + h) and SETD(A) = SETD(A + h).
(5)
Obviously, ETD(A, h) â‰¤ min(m, nâˆ’1) and ETD(A, h) â‰¤ SETD(A, h) â‰¤ min(m, n)
We now show
Lemma 2. We have ETD(A, h) â‰¤ SETD(A, h) â‰¤ ETD(A, h) + 1 and therefore
ETD(A) â‰¤ SETD(A) â‰¤ ETD(A) + 1.
Proof. The fact ETD(A, h) â‰¤ SETD(A, h) follows from the deï¬nitions. Let S âŠ†
[m] be a specifying set for h with respect to A. Then for T := {a âˆˆ A | (âˆ€i âˆˆ
S)hi = ai} we have t := |T| â‰¤ 1. If t = 0 or h âˆˆ A then S is a strong specifying
set for h with respect to A. If t = 1 and h (cid:54)âˆˆ A then for the element a âˆˆ T there
is j âˆˆ [m] such that aj (cid:54)= hj and then S âˆª {j} is a strong specifying set for h
with respect to A. This proves that SETD(A, h) â‰¤ ETD(A, h) + 1.
(cid:117)(cid:116)
The other claims follows immediately.
Obviously, for any B âŠ† A
ETD(B) â‰¤ ETD(A),
SETD(B) â‰¤ SETD(A).
(6)
2.4 Hitting Set
In this section we deï¬ne the hitting set for A.
A hitting set for A is a set S âŠ† [m] such that for every non-zero element
a âˆˆ A there is j âˆˆ S such that aj = 1. That is, S hits every element in A except
the zero element (if it exists). The size of the minimum size hitting set for A is
denoted by HS(A).
We now show
Lemma 3. We have HS(A) = SETDz(A). In particular, SETD(A, h) = HS(A+
h) and SETD(A) = maxhâˆˆBm HS(A + h).
Proof. If 0 âˆˆ A then SETDz(A) is the minimum size of a set S such that
{a âˆˆ A | (âˆ€i âˆˆ S)ai = 0} = {0} and if 0 (cid:54)âˆˆ A then it is the minimum size of a
set S such that {a âˆˆ A | (âˆ€i âˆˆ S)ai = 0} = âˆ…. Therefore the set S hits all the
nonzero elements in A.
(cid:117)(cid:116)
The other results follow from (5) and the deï¬nition of SETD.
6
2.5 Density of a Set
In this section we deï¬ne our new measure DEN of a set.
Let A = {a(1), . . . , a(n)} âŠ† Bm. We deï¬ne MAJ(A) âˆˆ Bm such that MAJ(A)i =
1 if the number of ones in (a(1)
) is greater or equal the number of zeros
and MAJ(A)i = 0 otherwise. We denote by MAX(A) the maximum number of
ones in (a(1)
) over all i = 1, . . . , m. Let
,Â·Â·Â· , a(n)
,Â·Â·Â· , a(n)
MAMI(A) = min
hâˆˆBm
MAX(A + h) = MAX(A + MAJ(A)).
(7)
For j âˆˆ [m] and Î¾ âˆˆ {0, 1} let Aj,Î¾ = {a âˆˆ A | aj = Î¾}. Then
MAMI(A) = max
min(|Aj,0|,|Aj,1|).
We deï¬ne the density of a set A âŠ† Bm by
DEN(A) = max
BâŠ†A
|B| âˆ’ 1
MAMI(B)
(8)
(9)
Notice that since every j âˆˆ [m] can hit at most MAX(A) elements in A we
have
HS(A) â‰¥ |A| âˆ’ 1
MAX(A)
(10)
3 Bounds for OPT
In this section we give upper and lower bounds for OPT.
3.1 Lower Bound
Moshkov results in [10, 13] and the information theoretic bound in (1) give the
following lower bound. We give the proof in Appendix A for completeness.
Lemma 4.
[10, 13] Let A âŠ† Bm be any set. Then
OPT(A) â‰¥ max(ETD(A), log |A|).
Many lower bounds in the literature for OPT(A) are based on ï¬nding a
subset B âŠ† A such that for each query there is an answer that eliminates at
most small fraction E of B. Then (|B|âˆ’ 1)/E is a lower bound for OPT(A). The
best possible bound that one can get using this technique is exactly DEN(A)
(Lemma 5), the density deï¬ned in Section 2.5. Lemma 6 shows that the lower
bound ETD(A) for OPT(A) exceeds any such bound.
In Appendix A we prove
Lemma 5. We have OPT(A) â‰¥ DEN(A).
Lemma 6. We have ETD(A) â‰¥ DEN(A) âˆ’ 1.
Proof. By (7) and (9) there is B âŠ† A such that
DEN(A) =
|B| âˆ’ 1
MAMI(B)
|B| âˆ’ 1
MAX(B + h)
where h = MAJ(B). Then
(11)
ETD(A)
(2)â‰¥ ETD(B, h)
(6)â‰¥ ETD(B)
L2â‰¥ SETD(B, h) âˆ’ 1 L3= HS(B + h) âˆ’ 1
(10)â‰¥
|B| âˆ’ 1
MAX(B + h)
âˆ’ 1
(11)
= DEN(A) âˆ’ 1.(cid:117)(cid:116)
In Appendix A we also prove
Lemma 7. We have
ETD(A) â‰¤ ln|A| Â· DEN(A) + 1.
It is also easy to see (by standard analysis using Chernoï¬€ Bound) that for a
random uniform A, with positive probability, DEN(A) = O(1) and ETD(A) =
Î˜(log |A|). See the proof sketch in Appendix A. So the bound in Lemma 7 is
asymptotically best possible.
3.2 Upper Bounds
Moshkov [10, 13] proved the following upper bound. We gave the proof in the
Appendix B for completeness.
Lemma 8.
[10, 13] Let A âŠ† {0, 1}m of size n. Then
OPT(A) â‰¤ ETD(A) +
ETD(A)
log ETD(A)
log n â‰¤ 2 Â· ETD(A)
log ETD(A)
log n.
In [10,13], Moshkov gave an example of a n-set AE âŠ† {0, 1}m with ETD(AE) =
E and OPT(AE) = â„¦((E/ log E) log n). So the upper bound in the above lemma
is the best possible.
4 Polynomial Time Approximation Algorithm
Given a a set A âŠ† Bm. Can one construct an algorithm that ï¬nds a hidden a âˆˆ A
with OPT(A) queries? Obviously, with unlimited computational power this can
be done so the question is: How close to OPT(A) can one get when polynomial
time poly(m, n) is allowed for the construction?
8
An exponential time algorithm follows from the following
OPT(A) = min
iâˆˆ[m]
max(OPT(Ai,0), OPT(Ai,1))
where Ai,Î¾ = {a âˆˆ A | ai = Î¾}. This algorithm runs in time at least m! â‰¥ (m/e)m.
See also [3, 7].
Can one give a better exponential time algorithm? In what follows (Theo-
rem 1) we use Moshkov [10, 13] result (Lemma 8) to give a better exponential
time approximation algorithm. In Appendix B we give another simple proof
of the Moshkov [10, 13] result that in practice uses less number of specifying
sets. When the extended teaching dimension is constant, the algorithm is O(1)-
approximation algorithm and runs in polynomial time.
Theorem 1. Let A be a class of sets A âŠ† Bm of size n. If there is an algorithm
that for any h âˆˆ Bm and any A âˆˆ A gives a specifying set for h with respect to
A of size at most E in time T then there is an algorithm that for any A âˆˆ A
constructs a decision tree for A of depth at most
E +
log E
log n â‰¤ E +
log E
OPT(A)
queries and runs in time O(T log n + nm).
Proof. Follows immediately from Moshkov algorithm [10, 13]. See Appendix B.(cid:117)(cid:116)
The following result immediately follows from Theorem 1.
Theorem 2. Let A âŠ† Bm be a n-set. There is an algorithm that ï¬nds the hidden
column in time
(cid:18) m
(cid:19)
Â· ETD(A) Â· n log n
and asks at most
ETD(A)
2 Â· ETD(A) Â· log n
log ETD(A)
â‰¤ 2 Â· min(ETD(A), log n)
log ETD(A)
OPT(A)
queries.
In particular, if ETD(A) is constant then the algorithm is O(1)-approximation
algorithm that runs in polynomial time.
Proof. To ï¬nd a specifying set for h with respect to A we exhaustively check
each ETD(A) row of A. Each check takes time n. Since the algorithm asks at
most ETD(A) Â· log n queries, the time complexity is as stated in the Theorem.
Can one do it in poly(m, n) time? Hyaï¬l and Rivest, [11], show that the
problem of ï¬nding OPT is NP-Complete. The reduction of Laber and Nogueira,
[12], of set cover to this problem with the inapproximability result of Dinur and
9
Steurer [6] for set cover implies that it cannot be approximated to (1âˆ’ o(1))Â· ln n
unless P=NP.
In [4], Arkin et al. showed that (the AMMRS-algorithm) if at the ith query
the algorithm chooses an index j that partitions the current node set (the ele-
ments in A that are consistent with the answers until this node) A as evenly as
possible, that is, that maximizes min(|{a âˆˆ A|aj = 0}|,|{a âˆˆ A|aj = 1}|), then
the query complexity is within a factor of (cid:100)log n(cid:101) from optimal. The AMMRS-
algorithm, [4], runs in time poly(m, n). Moshkov [4, 14] analysis shows that this
algorithm is ln n-approximation algorithm and therefore is optimal. In this sec-
tion we will give a simple proof.
In [10, 13], Moshkov gave a simple ETD(A)-approximation algorithm (Al-
gorithm MEMB-HALVING-1 in [10]). He then gave another algorithm that
achieves the query complexity in Lemma 8 (Algorithm MEMB-HALVING-2
in [10]). This is within a factor of
2 Â· min(ETD(A), log n)
log ETD(A)
from optimal. This is better than the ratio ln n, but, unfortunately, both algo-
rithms require ï¬nding a minimum size specifying set and the problem of ï¬nding
a minimum size specifying set for h is NP-Hard, [2, 8, 15].
Can one achieve a O(ETD(A))-approximation. In the following we give a sur-
prising result. We show that the AMMRS-algorithm is (ln 2)ETD(A)-approximation
algorithm. We also show that no better ratio can be achieved unless P=NP.
Theorem 3. The AMMRS-algorithm runs in time O(mn) and ï¬nds the hidden
element a âˆˆ A with at most
DEN(A) Â· ln(n) â‰¤ min((ln 2)DEN(A), ln n) Â· OPT(A)
â‰¤ min((ln 2)(ETD(A) + 1), ln n) Â· OPT(A)
queries.
Proof. Let B be any subset of A. Then,
DEN(B)
(9)â‰¥ |B| âˆ’ 1
MAMI(B)
and therefore
MAMI(B) â‰¥ |B| âˆ’ 1
DEN(B)
â‰¥ |B| âˆ’ 1
DEN(A)
Since the AMMRS-algorithm chooses at each node in the decision tree the
index j that maximizes min(|Bj,0|,|Bj,1|) where Bj,Î¾ = {a âˆˆ B|aj = Î¾} and B
is the set of elements in A that are consistent with the answers until this node,
we have
max(|Bj,0|,|Bj,1|) âˆ’ 1 = |B| âˆ’ 1 âˆ’ min(|Bj,0|,|Bj,1|)
(8)
= |B| âˆ’ 1 âˆ’ MAMI(B) â‰¤ (|B| âˆ’ 1)
(cid:18)
1 âˆ’
DEN(A)
(cid:19)
10
Therefore, for a node v of depth h in the decision tree, the set B(v) of elements
in A that are consistent with the answers until this node contains at most
(cid:19)h
+ 1
(cid:18)
(|A| âˆ’ 1)
1 âˆ’
DEN(A)
elements. Therefore the depth of the tree is at most
DEN(A) ln|A|.(cid:117)(cid:116)
We now show that our approximation algorithm is optimal
Theorem 4. Let  be any constant. There is no polynomial time algorithm that
ï¬nds the hidden element with less than (1 âˆ’ )DEN(A) Â· ln|A| unless P=NP.
Proof. Suppose such an algorithm exists. Then
(1 âˆ’ )DEN(A) ln|A| L5â‰¤ (1 âˆ’ ) ln|A|OPT(A).
That is, the algorithm is also (1 âˆ’ ) ln|A|-approximation algorithm. Laber and
Nogueira, [12] gave a polynomial time algorithm reduction of minimum depth
decision tree to set cover and Dinur and Steurer [6] show that there is no poly-
nomial time (1 âˆ’ o(1)) Â· ln|A| for set cover unless P=NP. Therefore, such an
(cid:117)(cid:116)
algorithm implies P=NP.
5 Applications to Disjunction of Predicates
In this section we apply the above results to learning the class of disjunctions of
predicates from a set of predicates F from membership queries [5].
Let C = {f1, . . . , fn} be a set of boolean functions fi : X â†’ {0, 1} where
X = {x1, . . . , xm}. Let AC = {(fi(x1), . . . , fi(xm)) | i = 1, . . . , n}. We will write
OPT(AC), ETD(AC), etc. as OPT(C), ETD(C), etc.
Let F be a set of boolean functions (predicates) over a domain X. We consider
the class of functions Fâˆ¨ := {âˆ¨fâˆˆSf | S âŠ† F}.
5.1 An Equivalence Relation Over Fâˆ¨
In this section, we present an equivalence relation over Fâˆ¨ and deï¬ne the repre-
sentatives of the equivalence classes. This enables us in later sections to focus on
the representative elements from Fâˆ¨. Let F be a set of boolean functions over
the domain X. The equivalence relation = over Fâˆ¨ is deï¬ned as follows: two
disjunctions F1, F2 âˆˆ Fâˆ¨ are equivalent (F1 = F2) if F1 is logically equal to F2.
In other words, they represent the same function (from X to {0, 1}). We write
F1 â‰¡ F2 to denote that F1 and F2 are identical; that is, they have the same
representation. For example, consider f1, f2 : {0, 1} â†’ {0, 1} where f1(x) = 1
and f2(x) = x. Then, f1 âˆ¨ f2 = f1 but f1 âˆ¨ f2 (cid:54)â‰¡ f1.
11
We denote by Fâˆ—
âˆ¨ the set of equivalence classes of = and write each equiv-
alence class as [F ], where F âˆˆ Fâˆ¨. Notice that if [F1] = [F2], then [F1 âˆ¨ F2] =
[F1] = [F2]. Therefore, for every [F ], we can choose the representative element
to be GF := âˆ¨F (cid:48)âˆˆSF (cid:48) where S âŠ† F is the maximum size set that satisï¬es
âˆ¨S := âˆ¨fâˆˆSf = F . We denote by G(Fâˆ¨) the set of all representative ele-
ments. Accordingly, G(Fâˆ¨) = {GF | F âˆˆ Fâˆ¨}. As an example, consider the
set F consisting of four functions f11, f12, f21, f22 : {1, 2}2 â†’ {0, 1} where
fij(x1, x2) = [xi â‰¥ j] where [xi â‰¥ j] = 1 if xi â‰¥ j and 0 otherwise. There
2 := Fâˆ¨ and ï¬ve representative functions in G(Fâˆ¨):
are 24 = 16 elements in Ray2
G(Fâˆ¨) = {f11âˆ¨f12âˆ¨f21âˆ¨f22, f12âˆ¨f22, f12, f22, 0} (where 0 is the zero function).
5.2 A Partial Order Over Fâˆ¨ and Hasse Diagram
In this section, we deï¬ne a partial order over Fâˆ¨ and present related deï¬nitions.
The partial order, denoted by â‡’, is deï¬ned as follows: F1â‡’F2 if F1 logically
implies F2. Consider the Hasse diagram H(Fâˆ¨) of G(Fâˆ¨) for this partial order.
The maximum (top) element in the diagram is Gmax := âˆ¨fâˆˆF f . The minimum
(bottom) element is Gmin := âˆ¨fâˆˆâˆ…f , i.e., the zero function. Figures 3 and 4 shows
an illustration of the Hasse diagram.
In a Hasse diagram, G1 is a descendant (resp., ascendent) of G2 if there is a
(nonempty) downward path from G2 to G1 (resp., from G1 to G2), i.e., G1â‡’G2
(resp., G2â‡’G1) and G1 (cid:54)= G2. G1 is an immediate descendant of G2 in H(Fâˆ¨)
if G1â‡’G2, G1 (cid:54)= G2 and there is no G âˆˆ G(Fâˆ¨) such that G (cid:54)= G1, G (cid:54)= G2
and G1â‡’Gâ‡’G2. G1 is an immediate ascendant of G2 if G2 is an immediate
descendant of G1.
We denote by De(G) and As(G) the sets of all the immediate descendants and
immediate ascendants of G, respectively. The neighbours set of G is Ne(G) =
De(G) âˆª As(G). We further denote by DE(G) and AS(G) the sets of all Gâ€™s
descendants and ascendants, respectively.
Deï¬nition 1. The degree of G is deg(G) = |Ne(G)| and the degree deg(Fâˆ¨) of
Fâˆ¨ is maxGâˆˆG(Fâˆ¨) deg(G).
For G1 and G2, we deï¬ne their lowest common ascendent (resp., greatest com-
mon descendant) G = lca(G1, G2) (resp., G = gcd(G1, G2)) to be the minimum
(resp., maximum) element in AS(G1) âˆ© AS(G2) (resp., DE(G1) âˆ© DE(G2)).
Lemma 9. Let G1, G2 âˆˆ G(Fâˆ¨). Then, lca(G1, G2) = G1 âˆ¨ G2.
G1 âˆ¨ G2 = G.
In particular, if G1, G2 are two distinct immediate descendants of G, then
The following result is from [5]
5.3 Witnesses
In this subsection we deï¬ne the term witness. Let G1 and G2 be elements in
G(Fâˆ¨). An element a âˆˆ X is a witness for G1 and G2 if G1(a) (cid:54)= G2(a).
For a class of boolean functions C over a domain X and a function G âˆˆ C
we say that a set of elements W âŠ† X is a witness set for G in C if for every
G(cid:48) âˆˆ C and G(cid:48) (cid:54)= G there is a witness in W for G and G(cid:48).
12
5.4 The Extended Teaching Dimension of Fâˆ¨
In this section we prove
Lemma 10. For every h : X â†’ {0, 1} if h (cid:59) Gmax then ETD(Fâˆ¨, h) = 1.
Otherwise, there is G âˆˆ G(Fâˆ¨) such that
ETD(Fâˆ¨, h) â‰¤ |De(G)| + HS(As(G) âˆ§ Â¯G) â‰¤ |Ne(G)| = deg(G)
where As(G) âˆ§ Â¯G = {s âˆ§ Â¯G | s âˆˆ As(G)}. In particular,
(cid:0)|De(G)| + HS(As(G) âˆ§ Â¯G)(cid:1) â‰¤ deg(Fâˆ¨).
ETD(Fâˆ¨) â‰¤ max
GâˆˆG(Fâˆ¨)
Proof. Let h : X â†’ {0, 1} be any function. If h (cid:59) Gmax then there is an
assignment a that satisï¬es h(a) = 1 and Gmax(a) = 0. Since for all G âˆˆ G(Fâˆ¨),
G â‡’ Gmax we have G(a) = 0. Therefore, the set {a} is a specifying set for h
with respect to Fâˆ¨ and ETD(Fâˆ¨, h) = 1.
Let h â‡’ Gmax. Consider any G âˆˆ G(Fâˆ¨) such that hâ‡’G and for every imme-
diate descendant G(cid:48) of G we have h (cid:59) G(cid:48). Now for every immediate descendent
G(cid:48) of G ï¬nd an assignment a such that G(cid:48)(a) = 0 and h(a) = 1. Then a is a
witness for h and G(cid:48). Therefore, a is also a witness for h and every descendant
of G(cid:48). Let A be the set of all such assignments, i.e., for every descendant of G
one witness. Then |A| â‰¤ |De(G)| and A is a witness set for h and all the descen-
dants of G. We note here that if h = 0 then G = Gmin which has no immediate
descendants and then A = âˆ….
Consider a hitting set B for As(G)âˆ§ Â¯G of size HS(As(G)âˆ§ Â¯G). Now for every
immediate ascendant G(cid:48)(cid:48) of G ï¬nd an assignment b âˆˆ B such that G(cid:48)(cid:48)(b)âˆ§ Â¯G(b) =
1. Then G(cid:48)(cid:48)(b) = 1 and G(b) = 0. Since G(b) = 0 we have h(b) = 0 and then b is
a witness for h and G(cid:48)(cid:48). Therefore, b is also a witness for h and every ascendant
of G(cid:48)(cid:48). Thus B is a witness set for h in all the ascendants of G.
Let G0 be any element in G(Fâˆ¨) (that is not a descendant or an ascendant).
Consider G1 = lca(G, G0). By Lemma 9, we have G1 = G âˆ¨ G0. Since G1 is an
ascendent of G there is a witness a âˆˆ B such that G1(a) = 1 and G(a) = 0.
Then G0(a) = 1, h(a) = 0 and a is a witness of h and G0. Therefore A âˆª B is
a specifying set for h with respect to G(Fâˆ¨). Since for every F âˆˆ Fâˆ¨ we have
F = GF âˆˆ G(Fâˆ¨), A âˆª B is also a specifying set for h with respect to Fâˆ¨.
Since
ETD(Fâˆ¨, h) â‰¤ |A| + |B| â‰¤ |De(G)| + HS(As(G) âˆ§ Â¯G)
the result follows.
In Appendix C we show that
ETD(Fâˆ¨) = max
GâˆˆG(Fâˆ¨)
(cid:0)|De(G)| + HS(As(G) âˆ§ Â¯G)(cid:1) .
(cid:117)(cid:116)
We could have replaced |De(G)| by HS(De(G)âˆ§ G), but Lemma 14 in Appendix
C shows that they are both equal.
The following result follows immediately from the proof of Lemma 10
Lemma 11. For any h : X â†’ {0, 1}, a specifying set for h with respect to Fâˆ¨
of size deg(Fâˆ¨) can be found in time O(nm).
By Theorem 1 we have
13
Theorem 5. There is an algorithm that learns Fâˆ¨ in time O(nm) and asks at
most
(cid:18) deg(Fâˆ¨)
log deg(Fâˆ¨)
(cid:19)
+ 1
OPT(Fâˆ¨)
deg(Fâˆ¨) +
deg(Fâˆ¨)
log deg(Fâˆ¨)
log n â‰¤
membership queries.
5.5 Learning Other Classes
If a specifying set of small size cannot be found in polynomial time then from
Theorem 2, 3 and Lemma 10, we have
Theorem 6. For a class C we have
1. There is an algorithm that learns C in time
(cid:18) m
(cid:19)
Â· ETD(C) Â· n log n
deg(C)
and asks at most
2 Â· ETD(C) Â· log n
log ETD(C))
â‰¤ 2 Â· min(ETD(C)), log n)
log ETD(C))
OPT(C)
membership queries.
In particular, when ETD(C) is constant the algorithm runs in polynomial
time and its query complexity is (asymptotically) optimal.
2. There is an algorithm that learns C in time O(nm) and asks at most
DEN(C) Â· ln(n) â‰¤ min((ln 2)DEN(C), ln n) Â· OPT(C)
â‰¤ min((ln 2)(ETD(C) + 1), ln n) Â· OPT(C)
membership queries.
References
1. Dana Angluin. Queries and concept learning. Machine Learning, 2(4):319â€“342,
1988.
2. Martin Anthony, Graham R. Brightwell, David A. Cohen, and John Shawe-Taylor.
On exact speciï¬cation by examples.
In Proceedings of the Fifth Annual ACM
Conference on Computational Learning Theory, COLT 1992, Pittsburgh, PA, USA,
July 27-29, 1992., pages 311â€“318, 1992.
14
3. Esther M. Arkin, Michael T. Goodrich, Joseph S. B. Mitchell, David M. Mount,
Christine D. Piatko, and Steven Skiena. Point probe decision trees for geometric
concept classes. In Algorithms and Data Structures, Third Workshop, WADS â€™93,
MontrÂ´eal, Canada, August 11-13, 1993, Proceedings, pages 95â€“106, 1993.
4. Esther M. Arkin, Henk Meijer, Joseph S. B. Mitchell, David Rappaport, and Steven
Int. J. Comput. Geometry Appl.,
Skiena. Decision trees for geometric models.
8(3):343â€“364, 1998.
5. Nader H. Bshouty, Dana Drachsler-Cohen, Martin T. Vechev, and Eran Yahav.
Learning disjunctions of predicates.
In Proceedings of the 30th Conference on
Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-10 July 2017, pages
346â€“369, 2017.
6. Irit Dinur and David Steurer. Analytical approach to parallel repetition. In Sym-
posium on Theory of Computing, STOC 2014, New York, NY, USA, May 31 -
June 03, 2014, pages 624â€“633, 2014.
7. M. R. Garey. Optimal binary identiï¬cation procedures. SIAM Journal on Applied
Mathematics, 23(2):173â€“186, 1971.
8. Sally A. Goldman and Michael J. Kearns. On the complexity of teaching. J.
Comput. Syst. Sci., 50(1):20â€“31, 1995.
9. Sally A. Goldman, Ronald L. Rivest, and Robert E. Schapire. Learning binary
In 30th Annual Symposium on
relations and total orders (extended abstract).
Foundations of Computer Science, FOCS 1989, pages 46â€“51.
10. Tibor HegedÂ¨us. Generalized teaching dimensions and the query complexity of
learning. In Proceedings of the Eigth Annual Conference on Computational Learn-
ing Theory, COLT 1995, Santa Cruz, California, USA, July 5-8, 1995, pages 108â€“
117, 1995.
11. Laurent Hyaï¬l and Ronald L. Rivest. Constructing optimal binary decision trees
is np-complete. Inf. Process. Lett., 5(1):15â€“17, 1976.
12. Eduardo Sany Laber and Loana Tito Nogueira. On the hardness of the minimum
height decision tree problem. Discrete Applied Mathematics, 144(1-2):209â€“212,
2004.
13. M. Y. Moshkov. On conditional tests. Problemy Kibernetiki. and Sov. Phys. Dokl.,
27(7):528â€“530, 1982.
14. Mikhail Ju. Moshkov. Greedy algorithm of decision tree construction for real data
tables. pages 161â€“168, 2004.
15. Ayumi Shinohara. Teachability in computational learning. New Generation Com-
put., 8(4):337â€“347, 1991.
16. Ayumi Shinohara and Satoru Miyano. Teachability in computational learning. In
Algorithmic Learning Theory, First International Workshop, ALT â€™90.
15
6 Appendix A
In this Appendix we give a proof of some lemmas
Lemma 1. We have
OPT(A) = OPT(A + h).
Proof. Since (A + h) + h = A, it is enough to prove that OPT(A + h) â‰¤ OPT(A).
Now given a decision tree T for A of depth OPT(A). For each node, v, in T
labeled with j, such that hj = 1, exchange the labels in their outgoing edges.
Then change the label of each leaf labeled with a to a + h. It is easy to show
(cid:117)(cid:116)
that the new tree is a decision tree for A + h.
Lemma 4. [10, 13] Let A âŠ† {0, 1}m be any set. Then
OPT(A) â‰¥ max(ETD(A), log |A|).
Proof. The lower bound log |A| is the information theoretic lower bound. We
now prove the other bound.
Let T be a decision tree for A = {a(1), . . . , a(n)} of minimum depth. Consider
the path P in T that at each level chooses the edge that is labeled with 0.
Let S be the set of labels in the internal nodes of P and a(j) be the label
of the leaf of P . Then a(j) is the only element in A that satisï¬es a(j)
i = 0
for all i âˆˆ S. Therefore S is a specifying set for 0 with respect to A. Thus
OPT(A) â‰¥ |S| â‰¥ ETDz(A). Now, by Lemma 1, for any h âˆˆ {0, 1}m we have
OPT(A) = OPT(A + h) â‰¥ ETDz(A + h) = ETD(A, h) and therefore OPT(A) â‰¥
(cid:117)(cid:116)
maxh ETD(A, h) = ETD(A).
Lemma 5. We have OPT(A) â‰¥ DEN(A).
Proof. Let B âŠ† A be a set such that
|B| âˆ’ 1
MAMI(B)
MAX(B + MAJ(B))
|B| âˆ’ 1
DEN(A)
(9)
(7)
For every query i âˆˆ [m] (what is â€œaiâ€?), the adversary answers MAJ(B)i. This
eliminates at most MAX(B+MAJ(B)) elements from B. Therefore the algorithm
is forced to ask at least (|B| âˆ’ 1)/MAX(B + MAJ(B)) queries.
(cid:117)(cid:116)
Lemma 7 We have
ETD(A) â‰¤ ln|A| Â· DEN(A) + 1.
Proof. There is h0 âˆˆ {0, 1}m such that
ETD(A)
L2â‰¤ SETD(A)
(4)
= SETD(A, h0) L3= HS(A + h0).
(12)
16
For any C âŠ† A we have
DEN(C)
(9)
= max
BâŠ†C
(7)â‰¥ max
BâŠ†C
|B| âˆ’ 1
MAMI(B)
|B| âˆ’ 1
MAX(B + h0)
|C| âˆ’ 1
MAX(C + h0)
and therefore, for any C âŠ† A we have
MAX(C + h0) â‰¥ |C| âˆ’ 1
DEN(C)
(9)â‰¥ |C + h0| âˆ’ 1
DEN(A)
(13)
We now consider the following sequence of subsets of A + h0, C0, C1, . . . , Ct
where C0 = A + h0 and the subset Ci+1 is deï¬ned by Ci as follows: Since (13)
is also true for Ci there is ji âˆˆ [m] such that ji hits at least (|Ci| âˆ’ 1)/DEN(A)
elements in Ci. Then Ci+1 contains all the elements in Ci that are not hit by ji.
Then
|Ci+1| âˆ’ 1 â‰¤ |Ci| âˆ’ |Ci| âˆ’ 1
DEN(A)
Therefore
|Ci| â‰¤ (|A| âˆ’ 1)
(cid:18)
âˆ’ 1 = (|Ci| âˆ’ 1)
1 âˆ’
DEN(A)
1 âˆ’
DEN(A)
+ 1.
Let Ct be the ï¬rst set in this sequence that satisï¬es Ct = âˆ… or Ct = {0}. Deï¬ne
X = {ji|i = 0, 1, . . . , tâˆ’1}. Then X is a hitting set for A+h0 of size t. Therefore,
by (12) we have
(cid:19)
(cid:18)
(cid:19)i
ETD(A) â‰¤ HS(A + h0) â‰¤ t â‰¤
ln
We now give proof sketch of
(cid:16)
ln(|A| âˆ’ 1)
1 âˆ’
DEN(A)
(cid:17)âˆ’1 + 1 â‰¤ DEN(A) Â· ln|A| + 1.
(cid:117)(cid:116)
Lemma 12. There is a set A âŠ† Bm of size n where m = poly(n) such that
ETD(A) = â„¦(log n) and DEN(A) = O(1).
Proof. Consider a random uniform set A âŠ† Bm of size n. The probability that
there are k = (log n)/2 entries i1, . . . , tk âˆˆ [m] such that no a âˆˆ A satisï¬es
(cid:19)(cid:18)
ai1 = ai2 = Â·Â·Â· = aik = 0 is (cid:18)m
(cid:19)n â‰¤ 1
1 âˆ’ 1
2k
17
Therefore, with probability at least 3/4, ETDz(A) â‰¥ k and then ETD(A) =
â„¦(log n).
The probability that some subset B âŠ† A of size |B| > 100 has MAMI(B) â‰¤
|B|/100 is at most
(cid:18) 1
(cid:19)m
2n
Therefore with probability at least 3/4, MAMI(B) â‰¥ |B|/100 and DEN(A) =
(cid:117)(cid:116)
O(1).
7 Appendix B
In this appendix we give the proof of Lemma 8 that is the same as the proof of
Lemma 3.2 in [10].
Lemma 8 [10, 13] Let A âŠ† {0, 1}m of size n. Then
OPT(A) â‰¤ ETD(A) +
ETD(A)
log ETD(A)
log n â‰¤ 2 Â· ETD(A)
log ETD(A)
log n.
Proof. Consider the algorithm in Figure 1. In Step 3, the algorithm deï¬nes a
hypothesis that is the bitwise majority of all the vectors in A(i,1). In Step 7 an
index y is found that maximizes the size of
(cid:110)
(cid:111)
A(i,k)
(y,fy) :=
g âˆˆ A(i,k) | gy = fy
Suppose the variable i (in the algorithm) gets the values 1, 2, . . . , t + 1 and for
each 1 â‰¤ i â‰¤ t the variable k gets the values 0, 1, 2, . . . , ki. Then the number of
membership queries asked by the algorithm is k1 + Â·Â·Â· + kt. We ï¬rst prove the
following
Claim For i = 1, . . . , t âˆ’ 1 we have
|A(i+1,1)| â‰¤ |A(i,1)|
max(2, ki)
Proof. Since S is a specifying set for h, either some y âˆˆ S satisï¬es hy (cid:54)= ay or
a is the only column in A that is consistent with h on S. Therefore, since h =
Majority(A(i,1)), we have
|A(i+1,1)| â‰¤ |A(i,1)|
(14)
Let D = A(i,1) and D(cid:48) = A(i+1,1). Suppose y1, . . . , yki are the queries that
were asked in the ith stage and let Î´j = ayj for j = 1, . . . , ki. Then
D(cid:48) = D(y1,Î´1),(y2,Î´2),...,(yki ,Î´ki )
18
and (disjoint union)
D = D(y1,Â¯Î´1) âˆª D(y1,Î´1),(y2,Â¯Î´2) âˆª Â·Â·Â· âˆª D(y1,Î´1),(y2,Î´2)...,(ykiâˆ’1,Î´kiâˆ’1),(yki ,Â¯Î´ki ) âˆª D(cid:48).
Let D(j) = D(y1,Î´1),(y2,Î´2)...,(yj ,Î´j ), the set of columns in D that are consistent
with the target column on the ï¬rst j assignments y1, . . . , yj. Then
D = D(0)
(y1,Â¯Î´1) âˆª D(1)
(y2,Â¯Î´2) âˆª Â·Â·Â· âˆª D(kiâˆ’1)
(yki ,Â¯Î´ki ) âˆª D(cid:48).
For 0 â‰¤ j â‰¤ ki âˆ’ 2, the fact that we took yj+1 for the (j + 1)th query and not
yki implies that |D(j)
)|. Therefore, for 0 â‰¤ j â‰¤ ki âˆ’ 2
(yj+1,hyj+1 )| â‰¤ |D(j)
| = |D(j)
(ykj ,hykj
| â‰¥ |D(j)
(yj+1,hyj+1 )
| = |D(j)
(ykj ,Î´kj )| â‰¥ |D(cid:48)|.
(ykj ,hykj
|D(j)
(yj+1,Î´j+1)
Therefore
|D| = |D(0)
(y1,Â¯Î´1)| + |D(1)
(y2,Â¯Î´2)| + Â·Â·Â· + |D(kiâˆ’1)
(yki ,Â¯Î´ki )| + |D(cid:48)| â‰¥ ki Â· |D(cid:48)|.
With (14), the result of the claim follows.
(cid:117)(cid:116)
Let zi = max(2, ki). Then
and therefore(cid:80)tâˆ’1
1 â‰¤ |A(t,1)| â‰¤
n(cid:81)t
i=1 zi
i=1 log zi â‰¤ log n. Now for E â‰¥ 4 and since E â‰¤ n
t(cid:88)
tâˆ’1(cid:88)
ki
ki = kt +
log zi
i=1
i=1
log zi
â‰¤ kt + max
â‰¤ E +
log E
ki
log n
log zi
log n â‰¤ 2E
log E
log n.
It is also easy to show that the above is also true for E = 2, 3.
We now prove the time complexity. Finding a specifying set at each iteration
of the While loop takes time T and the number of iterations in at most log n.
This takes T log n time. Now at the ï¬rst iteration we deï¬ne an array of length
|S| â‰¤ E that contains |A(i,1)
(z,hz)| for each z âˆˆ S. This takes at most |A(i,1)| Â· E
time. Now if we have such array for A(i,k)
(z,hz), we can ï¬nd y (in Step 7) in time E
and update the array for A(i,k+1) = A(i,k)
(y,hy) in time |A(i,k)\A(i,k)
(y,hy)|Â·E. Therefore
the time of the Repeat loop is at most 2|A(i,1)| Â· E. Since |A(i+1,1)| â‰¤ |A(i,1)|/2,
(cid:117)(cid:116)
the time of the While loop is at most 4n Â· E. This gives the result.
Algorithm: Find the hidden column a âˆˆ A.
19
1. i â† 1, k â† 0, A(1,1) â† A.
2. While |A(i,1)| â‰¥ 2 do
3. h â† Majority(A(i,1))
4. Find a specifying set S for h with respect to A(i,1)
5. Repeat
6.
(cid:12)(cid:12)(cid:12)A(i,k)
(cid:12)(cid:12)(cid:12)
7.
(z,hz )
k â† k + 1.
Find y â† arg minzâˆˆS
Ask query â€œWhat is ayâ€?
A(i,k+1) â† A(i,k)
S â† S\{y}.
8.
9.
10.
11. Until (hy (cid:54)= ay or |A(i,k+1)| = 1)
12. A(i+1,1) â† A(i,k+1), i â† i + 1, k â† 0
13. End While
14. Output the column in A(i,k).
(y,ay )
Fig. 1. An algorithm that ï¬nd the hidden column a âˆˆ A
We now give another proof
Proof. of Theorem 1 Consider the following algorithm. After the ith query,
the algorithm deï¬nes a set Ai âŠ† A of all the columns that are consistent with
the answers of the queries that were asked so far. Consider any 0 <  < 1. Now
the algorithm searches for a j âˆˆ [m] such that
|Ai| â‰¤ |{a âˆˆ Ai | aj = 0}| â‰¤ (1 âˆ’ )|Ai|.
If such j âˆˆ [m] exists then the algorithm asks â€œWhat is aj?â€. Let the answer
be Î¾. Deï¬ne Ai+1 = {a âˆˆ Ai | aj = Î¾}. Obviously, in that case,
|Ai+1| â‰¤ (1 âˆ’ )|Ai|.
If no such j âˆˆ [m] exists then the algorithm ï¬nds a specifying set Th for
h := Majority(Ai), where â€œMajorityâ€ is the bitwise majority function. Then
asks queries â€œWhat is ajâ€ for all j âˆˆ Th. If the answers are consistent with h on
Th then there is a unique column c âˆˆ Ai consistent with the answers and the
algorithm outputs the index of this column. Otherwise, there is j0 âˆˆ Th such
that aj0 (cid:54)= hj0. It is easy to see that in that case
|Ai+1| â‰¤ |Ai|.
(cid:24)
(cid:25)
Now when  = ln E/E we get
OPT(A) â‰¤ max
(cid:24) log n
(cid:25)(cid:19)
(cid:18)
log(1/)
log n
log(1/(1 âˆ’ ))
â‰¤ 2E
log E
log n.
20
The time complexity of this algorithm is O(T log n + mn).
(cid:117)(cid:116)
(cid:19)(cid:19)
log n
(cid:18) E log log E
log2 E
In fact one can prove the bound
(cid:18) E
OPT(A) â‰¤
E log log E
log2 E
+ o
log E
by substituting  = (ln E)/(E(1 + ln ln E/ ln E)).
8 Appendix C
In this Appendix we ï¬nd ETD(Fâˆ¨) exactly. We prove
ETD(Fâˆ¨) = max
GâˆˆG(Fâˆ¨)
|De(G)| + HS(As(G) âˆ§ Â¯G).
The following result is from [5].
Lemma 13. Let De(G) = {G1, G2, . . . , Gt} be the set of immediate descendants
of G. If a is a witness for G1 and G, then a is not a witness for Gi and G for
all i > 1. That is, G1(a) = 0, G(a) = 1, and G2(a) = Â·Â·Â· = Gt(a) = 1.
8.1 Teaching Dimension
The minimum size of a witness set for G in C is called the witness size and is
denoted by TD(C, G). The value
TD(C) := max
GâˆˆC
TD(C, G)
is called the teaching dimension of C, [8, 9, 16]. Obviously,
ETD(C, G) â‰¥ TD(C, G),
and ETD(C) â‰¥ TD(C).
8.2 The Proof
Lemma 14. For every G âˆˆ Fâˆ¨ we have
TD(Fâˆ¨, G) â‰¥ |De(G)| + HS(As(G) âˆ§ Â¯G).
In particular,
ETD(Fâˆ¨) = TD(Fâˆ¨) = max
GâˆˆG(Fâˆ¨)
(cid:0)|De(G)| + HS(As(G) âˆ§ Â¯G)(cid:1) .
Proof. Let B be a witness set for G in Ne(G). Take any G(cid:48) âˆˆ De(G). Then there
is a âˆˆ B such that G(cid:48)(a) = 0 and G(a) = 1. Since for any ascendent G(cid:48)(cid:48) of G we
have G(cid:48)(cid:48)(a) = 1, a is not a witness to G and any of its ascendants. By Lemma 13,
a cannot be a witness to any other descendent. In the similar way, a witness for
an ascendent of G and G cannot be a witness for any descendent of G and G.
Therefore,
21
TD(Fâˆ¨, G) â‰¥ TD(Ne(G), G) = TD(De(G), G) + TD(As(G), G)
(15)
Now let S be a witness set for G in As(G). Then for every G(cid:48)(cid:48) âˆˆ As(G) there is
a âˆˆ S such that G(cid:48)(cid:48)(a) = 1 and G(a) = 0 which is equivalent to G(cid:48)(cid:48)(a)âˆ§ Â¯G(a) = 1.
Therefore,
= |De(G)| + TD(As(G), G).
TD(As(G), G) â‰¥ HS(As(G) âˆ§ Â¯G).
This with (15) gives the result.
9 Appendix D
(cid:117)(cid:116)
9.1 Example of Classes
where fi1,i2,...,im(x1, . . . , xm) = (cid:86)m
Deï¬ne the class Raym
n . The functions are fi1,i2,...,im (x1, . . . , xm) : [n]m â†’ {0, 1}
j=1[xj â‰¥ ij]. It is easy to see that this class
contains O(nm) functions and its Hasse degree is 2m. See Ray2
See ï¬gure 4 for another example of F with Hasse degree 3.
4 in Figure 2.
Fig. 2. Hasse diagram of Ray2
[x2 â‰¥ i].
4. The functions are fi(x1, x2) = [x1 â‰¥ i] and gi(x1, x2) =
ğ‘“1,ğ‘“2,ğ‘“3,ğ‘“4,ğ‘”1,ğ‘”2,ğ‘”3,ğ‘”4ğ‘“2,ğ‘“3,ğ‘“4,ğ‘”2,ğ‘”3,ğ‘”4ğ‘“3,ğ‘“4,ğ‘”2,ğ‘”3,ğ‘”4ğ‘“4,ğ‘”2,ğ‘”3,ğ‘”4ğ‘”2,ğ‘”3,ğ‘”4ğ‘”3,ğ‘”4ğ‘”4ğ‘“2,ğ‘“3,ğ‘“4,ğ‘”3,ğ‘”4ğ‘“2,ğ‘“3,ğ‘“4,ğ‘”4ğ‘“2,ğ‘“3,ğ‘“4ğ‘“3,ğ‘“4ğ‘“4ğ‘“3,ğ‘“4,ğ‘”3,ğ‘”4ğ‘“4,ğ‘”3,ğ‘”4ğ‘“3,ğ‘“4,ğ‘”4ğ‘“4,ğ‘”422
Fig. 3. Hasse diagram of ...
Fig. 4. Hasse diagram when F = {f1, f2, f3, g1, g2, g3, h1, . . . , h5} of
functions
{1, 2, 3} Ã— {1, 2, 3} â†’ {0, 1} where fi(x1, y1) = [x1 â‰¥ i], gi(x1, x2) = [x2 â‰¥ i] and
hi(x1, x2) = [x1 + x2 â‰¥ i + 1].
ğ‘¥1âˆ¨ğ‘¥2âˆ¨ğ‘¥30ğ‘¥1âˆ¨ğ‘¥2ğ‘¥1âˆ¨ğ‘¥3ğ‘¥2âˆ¨ğ‘¥3ğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥1âˆ¨ğ‘¥2âˆ¨ğ‘¥1âˆ¨ğ‘¥20ğ‘¥1âˆ¨ğ‘¥2ğ‘¥1âˆ¨ğ‘¥2ğ‘¥2âˆ¨ğ‘¥1ğ‘¥1ğ‘¥2ğ‘¥1ğ‘¥1âˆ¨ğ‘¥2ğ‘¥2ğ‘“1,ğ‘“2,ğ‘“3,ğ‘”1,ğ‘”2,ğ‘”3,â„1,â„2,â„3,â„4,â„5ğ‘“2,ğ‘“3,ğ‘”2,ğ‘”3,â„2,â„3,â„4,â„5ğ‘“2,ğ‘“3,ğ‘”3,â„3,â„4,â„5ğ‘“2,ğ‘“3,â„4,â„5ğ‘“3,â„4,â„5ğ‘“3,â„5â„5ğ‘“3,ğ‘”2,ğ‘”3,â„3,â„4,â„5ğ‘”2,ğ‘”3,â„4,â„5ğ‘”3,â„4,â„5ğ‘”3,â„5ğ‘“3,ğ‘”3,â„3,â„4,â„5ğ‘“3,ğ‘”3,â„4,â„5â„4,â„5
